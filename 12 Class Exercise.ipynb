{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1]\n",
      "Trainings data: \n",
      " [(array([5.1, 3.5]), 1), (array([4.9, 3. ]), 1), (array([4.7, 3.2]), 1), (array([4.6, 3.1]), 1), (array([5. , 3.6]), 1), (array([5.4, 3.9]), 1), (array([4.6, 3.4]), 1), (array([5. , 3.4]), 1), (array([4.4, 2.9]), 1), (array([4.9, 3.1]), 1)]\n",
      "[0.30584655 0.31280655]\n"
     ]
    }
   ],
   "source": [
    "#Implement the Perceptron Learning Algorithm (PLA) similarly as demonstrated above for the petal length and petal width.\n",
    "#Run predictions on the full dataset using your calculated weights (output from the pla algorithm). \n",
    "#How many of the predictions are true?\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "#print(iris)\n",
    "#print(iris.data[0:1,:]) # show first line of matrix\n",
    "#print(len(iris.data),len(iris.target))\n",
    "#print(iris.target_names)\n",
    "#print(iris.target)\n",
    "\n",
    "class_labels = np.copy(iris.target);\n",
    "\n",
    "class_labels[class_labels != 0] = -1\n",
    "class_labels[class_labels == 0] = 1\n",
    "\n",
    "print(class_labels)\n",
    "\n",
    "trainings_data = [(d[:2], l) for d, l in zip(iris.data, class_labels)]\n",
    "print('Trainings data: \\n',trainings_data[:10])\n",
    "\n",
    "\n",
    "def activation_function(x):\n",
    "    \"\"\"\n",
    "    Step function to respond with y = 1 or -1\n",
    "    Parameter:\n",
    "    x: An x (numeric) value that will have a corresponding y value of 1 or -1\n",
    "    \"\"\"\n",
    "    if x < 0:\n",
    "        return -1\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def perceptron(inp, weights):\n",
    "    \"\"\"\n",
    "    Given a list of feature (x) values and a list of weights, \n",
    "    calculates the dot product of the 2 lists and returns 1 or -1 (fire or don't)\n",
    "    Parameters:\n",
    "    inp: vector of input predictors\n",
    "    weights: vector of weights to be ajusted for precise prediction of output.\n",
    "    \"\"\"\n",
    "    # This is the same as the dot product np.dot(i, w)\n",
    "    dot_product = sum([i * w for i, w in zip(inp, weights)])\n",
    "    output = activation_function(dot_product)\n",
    "    return output\n",
    "\n",
    "def predict(inp_vec, weights):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    inp_vec:\n",
    "        An input vector consisting of sepal length and width\n",
    "    weights:\n",
    "        A vector of same length as inp_vec, containing a weight for each input\n",
    "    return:\n",
    "        A class label, either 1 for 'setosa' or -1 for 'other'\n",
    "    \"\"\"\n",
    "    class_label_prediction = perceptron(inp_vec, weights)\n",
    "    return class_label_prediction\n",
    "\n",
    "def pla(training_data, no_iterations=2, eta=0.5):\n",
    "    \"\"\"\n",
    "    Find the proper weights to use in the perceptron based on data and target\n",
    "    Parameters:\n",
    "    training_data: list of vectors (features), as predictors zipped with a target value\n",
    "    no_iterations: number of times to adjust the weights to get them as close as possible to the optimal number\n",
    "    eta: the learning rate (prevent learning to go pendulum from one extreme error to the opposite)\n",
    "    \"\"\"\n",
    "    \n",
    "    dim = len(training_data[0][0]) # len = 2 (petal width and height)\n",
    "    weights =  np.random.random(dim) # error and weights (for x and y) start out as random numbers\n",
    "    \n",
    "    # initial_error\n",
    "    error = np.random.random()\n",
    "    weight_history = [np.copy(weights)]\n",
    "\n",
    "    for i in range(no_iterations):\n",
    "        #pdb.set_trace()\n",
    "        #breakpoint()\n",
    "        inp_vec, expected_label = training_data[i % len(training_data)]# get the next feature set and label (start over after reaching end)\n",
    "        perceptron_output = perceptron(inp_vec, weights) # perceptron output id a decimal between 0 and 1\n",
    "        error = expected_label - perceptron_output       # error \n",
    "        weights += eta * error * inp_vec # accumulate the weights\n",
    "        weight_history.append(np.copy(weights))\n",
    "        \n",
    "    return weights, weight_history\n",
    "\n",
    "learned_weights, weight_history = pla(trainings_data)\n",
    "print(learned_weights)\n",
    "count = 0\n",
    "\n",
    "for td in trainings_data:\n",
    "    if td[1] == predict(td[0], learned_weights):\n",
    "        count+=1\n",
    "\n",
    "#print('correct predictions ', count)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
